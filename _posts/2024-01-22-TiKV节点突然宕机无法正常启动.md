# TiKV 节点突然宕机无法正常启动

### 背景
18点53分突然收到报警，系统出现大量慢查询。感觉系统要出大事了，立刻打开 TiDB Dashboard查看，发现 tidb-server 的QPS等指标出现明显波动。
庆幸的是TiKV 单个节点宕机，对整个 TiDB 集群影响不大，感觉分布式的这点就是好啊。


### 故障分析
ssh进入故障节点查看日志，发现tikv_stderr.log出现多条错误日志如下：
```
[2024/01/22 17:54:59.962 +08:00] [FATAL] [lib.rs:465] ["to_commit 3553817 is out of range [last_index 3553815], raft_id: 8209515, region_id: 130773"] [backtrace="   0: tikv_util::set_panic_hook::{{closure}}
[2024/01/22 18:08:04.639 +08:00] [FATAL] [lib.rs:465] ["to_commit 813600 is out of range [last_index 813598], raft_id: 8131358, region_id: 139193"] [backtrace="   0: tikv_util::set_panic_hook::{{closure}}\n
......
```
通过pd-ctl 查询该 TiKV 的状态，一直显示“Down”。tidb集群自动重启机制也拉不起来。


### 解决方案
通过上网查询，问AI，上tidb官方论坛求助，发现有2帖子跟我的情况类似。通过命令
tiup ctl:v5.4.2 tikv --host 1xx.18.1xx.112:20160 bad-regions
查询有有问题的regions，不然道为什么返回错误：
```
Starting component `ctl`: /home/zmd-deploy/.tiup/components/ctl/v5.4.3/ctl tikv --host 10.0.0.40:20160 bad-regions
thread 'main' panicked at 'not implemented: only available for local mode', cmd/tikv-ctl/src/executor.rs:779:9
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
Error: exit status 101
```
平常没有学习，遇到问题的时候敲这些运维命令非常不熟练，很费时间。

有网友回应：这个意思大概就是：这个节点的日志到了last_index,但是要让提交到commit，这个commit超了本地的日志。就panic了。

官方文档也说：因为机器掉电导致 Raft 状态机丢失部分写入的情况。它可以在一个 TiKV 实例上将一些 Region 的副本设置为 Tombstone 状态，从而在重启时跳过这些 Region，避免因为这些 Region 的副本的 Raft 状态机损坏而无法启动服务。

一般情况下，可以先在 PD 上将 Region 的副本通过 remove-peer 命令删除掉：
```
pd-ctl>> operator add remove-peer <region_id> <store_id>
```
然后再用 tikv-ctl 在那个 TiKV 实例上将 Region 的副本标记为 tombstone 以便跳过启动时对他的健康检查：
```
tikv-ctl --data-dir /path/to/tikv tombstone -p 127.0.0.1:2379 -r <region_id>
```
尝试删，越删越多，删的速度赶不上增加的。是不是应该要怎么样先停掉节点？如何停？

又过了会惊喜地发现节点自己起来了，region和leader都在慢慢同步了。应该是经上面的两个步骤后生效了。

### 后记
第二天复盘，登录阿里云控制台，竟然发现有非预期运维事件：tikv节点实例因所在的底层宿主机突然出现了非预期的软硬件故障（如CPU、内存硬件损坏等）而被重启，同时实例重启后会自动被迁移至健康的宿主机。

原来是阿里云服务器硬盘故障导致的事故，确实捏了一把冷汗。年关近晚，正是业务高峰期。